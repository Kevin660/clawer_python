import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from urllib.parse import urlparse
def correcturl(url):
 return not (re.match("#.*",url or re.match("^javascript.*",url) or urlparse(url).scheme not in ["http","https"]))
def geturl():
  print("url:")
  url=input()
  if not correcturl(url):
    return geturl() 
  return url 
def getallhref(url):
  try:
    res=requests.get(url)
    soup=BeautifulSoup(res)
    return [x['href'] for x in soup.find_all('a')]    
  except:
    print("error")
    return -1

def clawer():
  visited=[]
  waitting=[]
  url=geturl()
  count=0
  waitting.append(url)
  while waitting != [] :

    href=getallhref(waitting[0])
    print(href)
    if (href):
     for x in href:
        if not(x in visited or x in waitting) and re.match("yuntech",x):
          waitting.append(x)
          count+=1
      

if __name__== '__main__':
  print("start")
  clawer()
