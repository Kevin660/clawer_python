import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin
from urllib.parse import urlparse
def correcturl(url):
 return not (re.match("#.*",url or re.match("^javascript.*",url) or urlparse(url).scheme not in ["http","https"]))
def geturl():
  print("url:")
  url=input()
  if not correcturl(url):
    return geturl() 
  return url 
def getallhref(res):
  soup=BeautifulSoup(res.text)
  
  return [x['href'] for x in soup.find_all('a',{"href":re.compile(".*")})] 
def fuc(result,res,tag):
  soup=BeautifulSoup(res.text)
  result.append(soup.find_all(tag))
  return True
def clawer():
  #define 
  exe=""
  visited=[]
  waitting=[]
  result=[]
  amount=0
  error=0
  success=0
  #start  
  print("start")
  url=geturl()
  print("tag:")
  tag=input()
  waitting.append(url)
  while waitting != [] :
    exe=waitting.pop(0)
    print(exe)
    try:
      res=requests.get(exe,timeout=2)
      print("linked")
    except:
      print("requests error")
      error+=1
      continue
    href=getallhref(res)
    if (fuc(result,res,tag)):
      success+=1
    if (href):
      for x in href:
        if not(x in visited or x in waitting) and re.match(".*yuntech",x) and correcturl(x) :
          waitting.append(urljoin(url,x))
          amount+=1
    print("amount:",amount)
    print("success:",success)
    print("error:",error)
  return result 
    
      

if __name__== '__main__':
  clawer()
  
